{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LogisticRegression",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uSlV6VFXnLIF",
        "colab_type": "text"
      },
      "source": [
        "In this notebook, we will cover logistic regression algorithm using gradient descent, along with Scikit-learn package.\n",
        "To illustate the algorithms, we use iris dataset for classification.\n",
        "Let's get after it!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aO-puAvxnONu",
        "colab_type": "text"
      },
      "source": [
        "Housekeeping"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6bpglGk-nPrK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from sklearn import datasets\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LU6DX3TVnaZS",
        "colab_type": "code",
        "outputId": "71ac239d-5dec-4568-96d2-ea4f09a24e18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "# Load the dataset\n",
        "iris = datasets.load_iris()\n",
        "\n",
        "# Identify X and y\n",
        "X = iris['data']\n",
        "y = iris['target']\n",
        "\n",
        "# Get the shape of X and y\n",
        "print(X.shape, y.shape)\n",
        "\n",
        "# There are 4 features in iris dataset.\n",
        "# For the sake of further calculations, let's make sure y has the shape of (150,1)\n",
        "y = y.reshape(len(y),1)\n",
        "\n",
        "# Get the updated shape of X and y\n",
        "print(X.shape, y.shape)\n",
        "\n",
        "# Now we are ready to start working with logistic regression!\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(150, 4) (150,)\n",
            "(150, 4) (150, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VOn3cenwoZpK",
        "colab_type": "text"
      },
      "source": [
        "Batch gradient descent for logistic regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IpkX3nYYok_D",
        "colab_type": "code",
        "outputId": "a5331c4f-c62c-47ef-b6ff-0010d475873b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# First we need to add the bias term X0 to the features\n",
        "X = np.c_[np.ones((len(X), 1)), X]  # add the term X0 = 1\n",
        "\n",
        "# Now get the shape of X\n",
        "print(\"Shape of X after adding bias term: {}\".format(X.shape))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of X after adding bias term: (150, 5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zbKV2R-hqJR_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Since logistic regression can classify two classes at a time, we got to update the y values.\n",
        "# Currently, there are 3 classes in iris dataset: 0,1,2\n",
        "# The code below takes class 2 as 1 and the rest as class 0\n",
        "\n",
        "y = (iris[\"target\"] == 2).astype(np.int)  # 1 if Iris virginica, else 0\n",
        "\n",
        "# Update the shape of y again\n",
        "y = y.reshape(len(y),1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aaqdM0PjoxW_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Before defining the gradient descent, let's look into the parameters we need to calculate for logistic regression.\n",
        "# 1) Guess initial parameters (weights) theta\n",
        "# 2) Calculate sigmoid function and a result probability for each observation\n",
        "# 3) Get y_predict as 1 if probability is greater than or equal to 0.5, else 0\n",
        "# 4) Calculate cost function\n",
        "# 5) Calculate gradient\n",
        "# 6) update theta as theta = theta - learning_rate * gradient\n",
        "# 7) Repeat steps 2 to 6 until convergence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "harRnKucrWjV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create training and test datasets\n",
        "test_ratio = 0.2\n",
        "total_size = len(X)\n",
        "\n",
        "test_size = int(total_size * test_ratio)\n",
        "train_size = total_size - test_size\n",
        "\n",
        "rnd_indices = np.random.permutation(total_size)\n",
        "\n",
        "X_train = X[rnd_indices[:train_size]]\n",
        "y_train = y[rnd_indices[:train_size]]\n",
        "X_test = X[rnd_indices[-test_size:]]\n",
        "y_test = y[rnd_indices[-test_size:]]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IMqj0-1Rq6cm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def LogisticGradientDescent(X, y, iterations, eta):\n",
        "\n",
        "  converged = False\n",
        "\n",
        "  m = X.shape[0]  # Training set\n",
        "  n = X.shape[1]  # feature size\n",
        "\n",
        "  iter = -1\n",
        "  # Initialize theta\n",
        "  theta = np.random.randn(n,1)   # (n,1) vector\n",
        "  costs = []\n",
        "\n",
        "  while not converged:\n",
        "    iter += 1\n",
        "    p_hat = 1/(1 + np.exp(-X.dot(theta)))  # probability for each instance\n",
        "\n",
        "    y_predict = [1 if x >= 0.5 else 0 for x in p_hat]  # predict class: 0 or 1\n",
        "\n",
        "    J = -1/m *(y.T.dot(np.log(p_hat)) + (1-y).T.dot(np.log(1-p_hat)))\n",
        "    costs.append(J)\n",
        "\n",
        "    gradient = -1/m *(X.T.dot(y - p_hat))\n",
        "\n",
        "    theta = theta - eta * gradient\n",
        "\n",
        "    if iter > iterations:\n",
        "      print(\"No. of iterations: {}\".format(iter))\n",
        "      converged = True\n",
        "\n",
        "  return theta, costs, y_predict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UA6Fa0Mgtrm4",
        "colab_type": "code",
        "outputId": "3a5a638f-91bd-49a4-9888-08be87c5375b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "# Now that parameters theta are updated for minimal cost function, let's calculate accuracy score on training set:\n",
        "\n",
        "theta, costs, y_predict  = LogisticGradientDescent(X_train, y_train, iterations = 5000, eta = 0.01)\n",
        "accuracy = np.mean(y_predict == y_train)\n",
        "print(\"Accuracy on training set: {:.2f}\".format(accuracy))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No. of iterations: 5001\n",
            "Accuracy on training set: 0.56\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWTk8O2zwYi7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot cost function vs. No. of iterations"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHRd9-42t4Sj",
        "colab_type": "code",
        "outputId": "c9ed7555-95c2-4a5f-dc4a-8aea1e3bc682",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Calculate accuracy on test set:\n",
        "p_hat = 1/(1 + np.exp(-X_test.dot(theta))) \n",
        "y_predict = [1 if x >= 0.5 else 0 for x in p_hat]\n",
        "\n",
        "accuracy = np.mean(y_predict == y_test)\n",
        "print(\"Accuracy on test set: {:.2f}\".format(accuracy))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy on test set: 0.51\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-MyLwbevaCt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ps3HpQrQwdI1",
        "colab_type": "text"
      },
      "source": [
        "Batch gradient descent for logistic regression with regularization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XgJp4voMwkb2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def LogisticGradientDescentwithRegularization(X, y, iterations, eta, C):\n",
        "\n",
        "  converged = False\n",
        "\n",
        "  m = X.shape[0]  # Training set\n",
        "  n = X.shape[1]  # feature size\n",
        "\n",
        "  iter = -1\n",
        "  # Initialize theta\n",
        "  theta = np.random.randn(n,1)   # (n,1) vector\n",
        "  costs = []\n",
        "\n",
        "  while not converged:\n",
        "    iter += 1\n",
        "    p_hat = 1/(1 + np.exp(-X.dot(theta)))  # probability for each instance\n",
        "\n",
        "    y_predict = [1 if x >= 0.5 else 0 for x in p_hat]  # predict class: 0 or 1\n",
        "\n",
        "    J = -C *(y.T.dot(np.log(p_hat)) + (1-y).T.dot(np.log(1-p_hat))) + 1/2* np.sum(np.square(theta))\n",
        "    costs.append(J)\n",
        "\n",
        "    gradient = -C *(X.T.dot(y - p_hat))\n",
        "    gradient[1:] = gradient[1:] + theta[1:]\n",
        "\n",
        "    theta = theta - eta * gradient\n",
        "\n",
        "    if iter > iterations:\n",
        "      print(\"No. of iterations: {}\".format(iter))\n",
        "      converged = True\n",
        "\n",
        "  return theta, costs, y_predict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "snbxqDdkzWn-",
        "colab_type": "code",
        "outputId": "fad01b28-17f5-404d-da1d-372f104c3682",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "theta, costs, y_predict  = LogisticGradientDescentwithRegularization(X, y, iterations = 5000, eta = 0.01, C = 0.1)\n",
        "accuracy = np.mean(y_predict == y_train)\n",
        "print(\"Accuracy on training set: {:.2f}\".format(accuracy))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No. of iterations: 5001\n",
            "Accuracy on training set: 0.56\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQHVPlIlzh9C",
        "colab_type": "code",
        "outputId": "0b3fc399-9aed-4570-ec18-4523edcda220",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Calculate accuracy on test set:\n",
        "p_hat = 1/(1 + np.exp(-X_test.dot(theta))) \n",
        "y_predict = [1 if x >= 0.5 else 0 for x in p_hat]\n",
        "\n",
        "accuracy = np.mean(y_predict == y_test)\n",
        "print(\"Accuracy on test set: {:.2f}\".format(accuracy))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy on test set: 0.53\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63vDrMXlzz0I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}